{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Compare model results and final model selection\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that mdoel on the holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "_Welcome back to the final lesson of the course. Up to this point we've learned about five separate algorithms, we've done some basic hyperparameter tuning, and we've saved the best model for each of the five algorithms._\n",
    "\n",
    "_In this lesson, we're going to compare that five models that were fit on the training set by evaluating them on the validation set. This will give us a view of how they do on data that they were not fit on. Then we will select the best model and evaluate it on the holdout test set to get an unbiased view of how this model will perform on completely unseen data._\n",
    "\n",
    "_Lets start by importing the packages we will need. I'll call out that we're importing accuracy, precision, and recall score calculators from `sklearn.metrics`. We'll cover what those mean in just a second. We're also importing a `time` package that will help us time how long it's taking each of these models to make predictions._\n",
    "\n",
    "_Lastly, we'll read in the features and labels for both our validation set and our test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "val_features = pd.read_csv('../val_features.csv')\n",
    "val_labels = pd.read_csv('../val_labels.csv', header=None)\n",
    "\n",
    "te_features = pd.read_csv('../test_features.csv')\n",
    "te_labels = pd.read_csv('../test_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models\n",
    "\n",
    "_Now, lets read in the models we have stored. For ease, since they all have the same name template, I'm going to read these in a loop and we're going to store these model objects in a dictionary. So we have this list of model names we will loop through and then we just need to enter the location within `joblib.load()`. We'll pass in the location of `../` then a bracket to be filled by `mdl` in the loop. And then ` model.pkl`. Then we will format that with `mdl`. So on the first loop this will read as `../LR model.pk` so it will find that model object, read it in and store it in the models dictionary with the key `LR`. This will just make it easier for us to call the models in a loop. So lets run this._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = joblib.load('../{}_model.pkl'.format(mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now, for evaluation metrics. If you took the Foundations course you'll be familiar with these performance metrics. If you haven't, these three performance metrics (accuracy, precision, and recall) are the standard evaluation metrics for classification problems._\n",
    "\n",
    "_Accuracy is just # correctly predicted over total # of examples._\n",
    "\n",
    "_Precision is # predicted as surviving that actually survived divided by total # predicted to survive. In other words, it says -- when the model predicted someone would survive, how often did they actually survive?_\n",
    "\n",
    "_Recall is # predicted as surviving that actually survived divided by the total number that actually survived. In other words, it says -- given that somebody actually survived, what is the likelihood the model correctly predicted they would survive?_\n",
    "\n",
    "_Notice that the numerator of precision and recall are the same, it's just the denominator that is different._\n",
    "\n",
    "_Ok, now we just have a function that's going to help us evaluate each of our 5 models on the validation set. Our function is called `evaluate_model` and it accepts the following arguments:_\n",
    "1. _Name of the model_\n",
    "2. _The model object itself_\n",
    "3. _Features (for the validation or test set)_\n",
    "4. _Labels (for the validation or test set)_\n",
    "\n",
    "_Now we're using this `time()` method that just stores the time when that given command was run. So in between start and end we're going to add our predict functionality. So recall that we talked about how all `sklearn` model objects have the same `.fit()` and `.predict()` API. So these model objects passed in here are all `sklearn` model objects. So we will just call `model.predict()` and pass in the features. What that will return is predictions given that model fit on the training set running on the features in the validation set._\n",
    "\n",
    "_So lets store that as `pred`, so now we have predictions under `pred` and labels under `labels`. So we can now compare them to generate performance metrics. That's what the next few lines do - generate accuracy, precision, and recall score using labels and predictions. Then it will print all of those out along with the time it takes to make these predictions at the very end._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n",
    "                                                                                   accuracy,\n",
    "                                                                                   precision,\n",
    "                                                                                   recall,\n",
    "                                                                                   round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now we have a models dictionary where the keys are the model names and the values are the stored model object. So we can loop through this dictionary. So we can say for `name` and `mdl` in `models.items()`, within that for loop we will call our `evaluate model` prediction and we pass in the model name, the model object, the validation features, and validation labels._\n",
    "\n",
    "_Now lets run that and explore our results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GB': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "               n_iter_no_change=None, presort='auto', random_state=None,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False),\n",
       " 'LR': LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       " 'MLP': MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "        beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "        learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "        n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "        random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "        validation_fraction=0.1, verbose=False, warm_start=False),\n",
       " 'RF': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'SVM': SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "   kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "   shrinking=True, tol=0.001, verbose=False)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_One thing to notice is how much longer Random Forest takes to make predictions. That's because it's run in parallel but each individual tree can be quite deep._\n",
    "\n",
    "_Note that depending on your problem you may have to optimize for precision over recall or vice versa. For instance, for a spam detector we would optimize for precision. In other words, if the model SAYS it's spam - it better be spam. The cost is potentially classifying non-spam as spam. On the other side, if this is a fraud detection model - you're likely to optimize for recall because missing one of those transactions could cost thousands or tens of thousands of dollars._\n",
    "\n",
    "_Further explore results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.816 / Precision: 0.841 / Recall: 0.697 / Latency: 0.9ms\n",
      "SVM -- Accuracy: 0.799 / Precision: 0.794 / Recall: 0.711 / Latency: 1.7ms\n",
      "MLP -- Accuracy: 0.81 / Precision: 0.828 / Recall: 0.697 / Latency: 1.3ms\n",
      "RF -- Accuracy: 0.799 / Precision: 0.857 / Recall: 0.632 / Latency: 4.9ms\n",
      "GB -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 1.9ms\n"
     ]
    }
   ],
   "source": [
    "for name, mdl in models.items():\n",
    "    evaluate_model(name, mdl, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set\n",
    "\n",
    "_Lastly, now that we have selected the best model, lets evaluate it on the test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 2.8ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Gradient Boosting', models['GB'], te_features, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
